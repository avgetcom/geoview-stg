{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование данных .tsv.gz в parquet.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный файл с 1000000 строками создан.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import gzip\n",
    "\n",
    "num_rows = 1000000\n",
    "\n",
    "# Генерируем данные\n",
    "data = {\n",
    "    'ID': range(1, num_rows + 1),\n",
    "    'Name': ['User' + str(i) for i in range(1, num_rows + 1)],\n",
    "    'Age': [random.randint(18, 65) for _ in range(1, num_rows + 1)],\n",
    "    'City': ['City' + str(random.randint(1, 10)) for _ in range(1, num_rows + 1)],\n",
    "    'Country': ['Country' + str(random.randint(1, 5)) for _ in range(1, num_rows + 1)],\n",
    "    'Gender': ['Male' if random.random() < 0.5 else 'Female' for _ in range(1, num_rows + 1)],\n",
    "    'Salary': [random.randint(30000, 100000) for _ in range(1, num_rows + 1)],\n",
    "    'Education': [random.choice(['Bachelor', 'Master', 'PhD']) for _ in range(1, num_rows + 1)],\n",
    "    'Employment': [random.choice(['Full-time', 'Part-time', 'Self-employed', 'Unemployed']) for _ in range(1, num_rows + 1)],\n",
    "    'Marital_Status': [random.choice(['Single', 'Married', 'Divorced', 'Widowed']) for _ in range(1, num_rows + 1)]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "test_file_path = 'data\\\\table-test.tsv.gz'\n",
    "\n",
    "with gzip.open(test_file_path, 'wt', encoding='utf-8') as f:\n",
    "    df.to_csv(f, sep='\\t', index=False)\n",
    "\n",
    "print(f'Исходный файл с {num_rows} строками создан.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "input_file = 'C:\\\\_pipeline\\\\beeline_2023-09_radio_by_s2.tsv.gz'\n",
    "output_file = 'C:\\\\_pipeline\\\\beeline_2023-09_radio_by_s2_p17.csv'\n",
    "# Размер блока данных (количество строк) для чтения\n",
    "chunksize = 1000\n",
    "\n",
    "with gzip.open(input_file, 'rt', encoding='utf-8') as file:\n",
    "    for i, chunk in enumerate(pd.read_csv(file, sep='\\t', chunksize=chunksize)):\n",
    "        df_chunk = pd.DataFrame(chunk)\n",
    "        df_filtred = df_chunk.loc[df_chunk['s2_level'] == 's2_p17']\n",
    "\n",
    "        if len(df_filtred) > 0:\n",
    "            break\n",
    "\n",
    "df_filtred = df_filtred.drop(columns=['s2_level'])\n",
    "df_filtred.to_csv(output_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  s2_level             s2_value CellType   fielddate  radio_devices  \\\n",
      "0   s2_p13  4627401733260181504      LTE  2023-09-04            NaN   \n",
      "1   s2_p13  4627401836339396608      LTE  2023-09-28            NaN   \n",
      "2   s2_p13  4627402351735472128      LTE  2023-09-16            NaN   \n",
      "3   s2_p13  4627402798412070912      LTE  2023-09-25            NaN   \n",
      "4   s2_p13  4627403897923698688      LTE  2023-09-18            NaN   \n",
      "\n",
      "   radio_devices_min  radio_devices_max  radio_devices_position  \\\n",
      "0               14.0               14.0                     NaN   \n",
      "1               15.0               15.0                     NaN   \n",
      "2               11.0               11.0                     NaN   \n",
      "3               23.0               42.0                     NaN   \n",
      "4               19.0               19.0                     NaN   \n",
      "\n",
      "   radio_measurements  radio_measurements_min  ...  SignalStrength_p80_max  \\\n",
      "0                 NaN                   245.0  ...              -98.000000   \n",
      "1                 NaN                  2393.0  ...              -86.000000   \n",
      "2                 NaN                   351.0  ...             -105.000000   \n",
      "3                 NaN                   752.0  ...              -75.991358   \n",
      "4                 NaN                   438.0  ...              -94.000000   \n",
      "\n",
      "   SignalStrength_p80_position  SignalStrength_p90  SignalStrength_p90_min  \\\n",
      "0                          NaN                 NaN                   -95.0   \n",
      "1                          NaN                 NaN                   -76.0   \n",
      "2                          NaN                 NaN                  -105.0   \n",
      "3                          NaN                 NaN                   -92.0   \n",
      "4                          NaN                 NaN                   -92.0   \n",
      "\n",
      "   SignalStrength_p90_max  SignalStrength_p90_position  SignalStrength_p95  \\\n",
      "0                 -95.000                          NaN                 NaN   \n",
      "1                 -76.000                          NaN                 NaN   \n",
      "2                -105.000                          NaN                 NaN   \n",
      "3                 -70.216                          NaN                 NaN   \n",
      "4                 -92.000                          NaN                 NaN   \n",
      "\n",
      "   SignalStrength_p95_min  SignalStrength_p95_max  SignalStrength_p95_position  \n",
      "0                   -94.0                   -94.0                          NaN  \n",
      "1                   -73.0                   -73.0                          NaN  \n",
      "2                  -102.0                  -102.0                          NaN  \n",
      "3                   -89.0                   -63.0                          NaN  \n",
      "4                   -91.0                   -91.0                          NaN  \n",
      "\n",
      "[5 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "input_file = 'C:\\\\_pipeline\\\\beeline_2023-09_radio_by_s2.tsv.gz'\n",
    "output_file = 'C:\\\\_pipeline\\\\beeline_2023-09_radio_by_s2.csv'\n",
    "\n",
    "# Размер блока данных (количество строк) для чтения\n",
    "chunksize = 1000\n",
    "\n",
    "with gzip.open(input_file, 'rt', encoding='utf-8') as file:\n",
    "        first_chunk = next(pd.read_csv(file, sep='\\t', chunksize=chunksize))\n",
    "\n",
    "first_chunk.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(first_chunk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Записано строк данных: 68685522\n",
      " Завершено\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import gzip\n",
    "\n",
    "input_file = 'C:\\\\_pipeline\\\\beeline_2023-09_radio_by_s2_month_binary.tsv.gz'\n",
    "output_file = 'C:\\\\_pipeline\\\\beeline_2023-09_radio_by_s2h17_month_binary.parquet.gz'\n",
    "\n",
    "# Определите размер блока (количество строк) для чтения\n",
    "chunksize = 1000000\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "try:\n",
    "    # Создайте схему данных из первой порции данных\n",
    "    with gzip.open(input_file, 'rt', encoding='utf-8') as file:\n",
    "        for i, chunk in enumerate(pd.read_csv(file, sep='\\t', chunksize=chunksize)):\n",
    "            df_chunk = pd.DataFrame(chunk)\n",
    "            df_filtred = df_chunk.loc[df_chunk['s2_level'] == 's2_p17']\n",
    "\n",
    "            if len(df_filtred) > 0:\n",
    "                break\n",
    "\n",
    "    df_filtred = df_filtred.drop(columns=['s2_level'])\n",
    "    table = pa.Table.from_pandas(df_filtred)\n",
    "    schema = table.schema\n",
    "    pq.write_table(table, output_file, compression='gzip')\n",
    "\n",
    "    rows_cnt = len(df_filtred)\n",
    "    print(f'Записано строк данных: {rows_cnt}', end='\\r')\n",
    "        \n",
    "    # Откройте gzip-файл и читайте данные по блокам\n",
    "    with gzip.open(input_file, 'rt', encoding='utf-8') as file:\n",
    "        with pq.ParquetWriter(output_file, schema, compression='gzip') as writer:\n",
    "            for i, chunk in enumerate(pd.read_csv(file, sep='\\t', chunksize=chunksize)):\n",
    "                df_chunk = pd.DataFrame(chunk)\n",
    "                df_filtred = df_chunk.loc[df_chunk['s2_level'] == 's2_p17']\n",
    "\n",
    "                if len(df_filtred) > 0:\n",
    "                    df_filtred = df_filtred.drop(columns=['s2_level'])\n",
    "                    table = pa.Table.from_pandas(df_filtred, schema=schema)\n",
    "                    writer.write_table(table)\n",
    "\n",
    "                    rows_cnt = rows_cnt + len(df_filtred)\n",
    "                    print(f'Записано строк данных: {rows_cnt}', end='\\r')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Произошла ошибка: {str(e)}')\n",
    "\n",
    "finally:\n",
    "    if 'table' in locals():\n",
    "        table = None\n",
    "\n",
    "print('\\n Завершено')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    s2_value CellType  SignalStrength_p50  \\\n",
      "1126873  1390848764293414912      LTE              -105.0   \n",
      "1126874  1391176878185775104      LTE                 NaN   \n",
      "1126875  1392402686548115456      LTE                 NaN   \n",
      "1126876  1393064275525763072      LTE                 NaN   \n",
      "1126877  1393640029179346944      LTE               -93.0   \n",
      "\n",
      "         SignalStrength_p50_other  \n",
      "1126873                     False  \n",
      "1126874                      True  \n",
      "1126875                      True  \n",
      "1126876                      True  \n",
      "1126877                     False  \n",
      "Число записей в Parquet-файле: 67812395\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "output_file = 'C:\\\\_pipeline\\\\beeline_2023-09_radio_by_s2h17_month_binary.parquet.gz'\n",
    "\n",
    "table = pq.read_table(output_file)\n",
    "# num_rows = 10  # Измените на нужное количество строк\n",
    "# subset = table.slice(0, num_rows)\n",
    "# df = subset.to_pandas()\n",
    "# print(df.head())\n",
    "\n",
    "df = table.to_pandas()\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "num_records_in_parquet = len(df)\n",
    "print(f'Число записей в Parquet-файле: {num_records_in_parquet}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__cinit__() got an unexpected keyword argument 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\#work\\.repos\\data-enginiring\\test.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/%23work/.repos/data-enginiring/test.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m             pq\u001b[39m.\u001b[39mwrite_table(table, output_file, compression\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/%23work/.repos/data-enginiring/test.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/%23work/.repos/data-enginiring/test.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m             \u001b[39mwith\u001b[39;00m pq\u001b[39m.\u001b[39;49mParquetWriter(output_file, table\u001b[39m.\u001b[39;49mschema, compression\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgzip\u001b[39;49m\u001b[39m'\u001b[39;49m, append\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39mas\u001b[39;00m writer:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/%23work/.repos/data-enginiring/test.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                 writer\u001b[39m.\u001b[39mwrite_table(table)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/%23work/.repos/data-enginiring/test.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mЗавершено\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\parquet\\core.py:1016\u001b[0m, in \u001b[0;36mParquetWriter.__init__\u001b[1;34m(self, where, schema, filesystem, flavor, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, compression_level, use_byte_stream_split, column_encoding, writer_engine_version, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, write_page_index, **options)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata_collector \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mmetadata_collector\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1015\u001b[0m engine_version \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mV2\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m-> 1016\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m _parquet\u001b[39m.\u001b[39;49mParquetWriter(\n\u001b[0;32m   1017\u001b[0m     sink, schema,\n\u001b[0;32m   1018\u001b[0m     version\u001b[39m=\u001b[39;49mversion,\n\u001b[0;32m   1019\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   1020\u001b[0m     use_dictionary\u001b[39m=\u001b[39;49muse_dictionary,\n\u001b[0;32m   1021\u001b[0m     write_statistics\u001b[39m=\u001b[39;49mwrite_statistics,\n\u001b[0;32m   1022\u001b[0m     use_deprecated_int96_timestamps\u001b[39m=\u001b[39;49muse_deprecated_int96_timestamps,\n\u001b[0;32m   1023\u001b[0m     compression_level\u001b[39m=\u001b[39;49mcompression_level,\n\u001b[0;32m   1024\u001b[0m     use_byte_stream_split\u001b[39m=\u001b[39;49muse_byte_stream_split,\n\u001b[0;32m   1025\u001b[0m     column_encoding\u001b[39m=\u001b[39;49mcolumn_encoding,\n\u001b[0;32m   1026\u001b[0m     writer_engine_version\u001b[39m=\u001b[39;49mengine_version,\n\u001b[0;32m   1027\u001b[0m     data_page_version\u001b[39m=\u001b[39;49mdata_page_version,\n\u001b[0;32m   1028\u001b[0m     use_compliant_nested_type\u001b[39m=\u001b[39;49muse_compliant_nested_type,\n\u001b[0;32m   1029\u001b[0m     encryption_properties\u001b[39m=\u001b[39;49mencryption_properties,\n\u001b[0;32m   1030\u001b[0m     write_batch_size\u001b[39m=\u001b[39;49mwrite_batch_size,\n\u001b[0;32m   1031\u001b[0m     dictionary_pagesize_limit\u001b[39m=\u001b[39;49mdictionary_pagesize_limit,\n\u001b[0;32m   1032\u001b[0m     store_schema\u001b[39m=\u001b[39;49mstore_schema,\n\u001b[0;32m   1033\u001b[0m     write_page_index\u001b[39m=\u001b[39;49mwrite_page_index,\n\u001b[0;32m   1034\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[0;32m   1035\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_open \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyarrow\\_parquet.pyx:1714\u001b[0m, in \u001b[0;36mpyarrow._parquet.ParquetWriter.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __cinit__() got an unexpected keyword argument 'append'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import gzip\n",
    "\n",
    "input_file = 'C:\\\\_pipeline\\\\beeline_2023-09_radio_by_s2.tsv.gz'\n",
    "output_file = 'C:\\\\_pipeline\\\\beeline_2023-09_radio_by_s2.parquet.gz'\n",
    "\n",
    "# Определите размер блока (количество строк) для чтения\n",
    "chunksize = 100000\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "try:\n",
    "    # Запись первого блока данных\n",
    "    with gzip.open(input_file, 'rt', encoding='utf-8') as file:\n",
    "        first_chunk = next(pd.read_csv(file, sep='\\t', chunksize=chunksize))\n",
    "        # schema = pa.schema([pa.field(col, pa.string()) for col in first_chunk.columns])\n",
    "        table = pa.Table.from_pandas(first_chunk)\n",
    "        schema = table.schema\n",
    "        pq.write_table(table, output_file, compression='gzip')\n",
    "        \n",
    "\n",
    "    # Запись данных по блокам\n",
    "    with gzip.open(input_file, 'rt', encoding='utf-8') as file:\n",
    "        with pq.ParquetWriter(output_file, schema, compression='gzip') as writer:\n",
    "            for i, chunk in enumerate(pd.read_csv(file, sep='\\t', chunksize=chunksize)):\n",
    "                df_chunk = pd.DataFrame(chunk)\n",
    "                table = pa.Table.from_pandas(df_chunk, schema=schema)\n",
    "                writer.write_table(table)\n",
    "\n",
    "                print(f'Записано строк данных: {(i + 1) * chunksize}', end='\\r')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Произошла ошибка: {str(e)}')\n",
    "\n",
    "finally:\n",
    "    # Закройте выходной файл в любом случае, чтобы избежать утечек ресурсов\n",
    "    if 'table' in locals():\n",
    "        table = None\n",
    "\n",
    "    print('Завершено')\n",
    "\n",
    "# Проверка записанных данных\n",
    "table = pq.read_table(output_file)\n",
    "\n",
    "num_rows_to_read = 10  # Измените на нужное количество строк\n",
    "subset = table.head(num_rows_to_read)\n",
    "\n",
    "df_subset = subset.to_pandas()\n",
    "print(df_subset)\n",
    "\n",
    "# Проверка записанных данных\n",
    "table = pq.read_table(output_file)\n",
    "df = table.to_pandas()\n",
    "\n",
    "print(df.head(num_rows_to_read))\n",
    "\n",
    "num_records_in_parquet = len(df)\n",
    "print(f'Записано строк: {num_records_in_parquet}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
